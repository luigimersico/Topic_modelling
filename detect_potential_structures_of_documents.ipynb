{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rDYTs7JHtQrqIeb_zWSTM1VdXaJvCApB",
      "authorship_tag": "ABX9TyOwNf8Of3tos3nd9OWqWE35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigimersico/Topic_modelling/blob/main/detect_potential_structures_of_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas pdfplumber spacy scikit-learn matplotlib pyPDF2 xlsxwriter PyMuPDF keyboard"
      ],
      "metadata": {
        "id": "zagHxI11LGpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "miZRsxjzK92U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "import csv\n",
        "import re\n",
        "import xlsxwriter\n",
        "import fitz\n",
        "import pathlib\n",
        "import keyboard\n",
        "import time\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " activites: 1) read multiple .pdf file of scientific article. Articles have authors, title, year of publications and corpus. 2) create a variable storing the name of the file. 3) extract from the pdf files title, authors, year of publications and corpus 3) dataframe file with 4 columns: 1 ID of article 2 year of publications 3) authors 4) title 5) corpus. Add the extracted text in the right column of csv file"
      ],
      "metadata": {
        "id": "snmCnZSMUy1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory where your PDF files are located\n",
        "pdf_folder_path = '/content/drive/MyDrive/Colab_Notebooks/input_data/example'\n",
        "# Specify the directory to save the converted text files\n",
        "text_output_folder = '/content/drive/MyDrive/Colab_Notebooks/output_data'"
      ],
      "metadata": {
        "id": "6XJWrp5oU1kZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_files(folder):\n",
        "    files = [f for f in os.listdir(folder) if f.endswith(\".pdf\")]\n",
        "    return files"
      ],
      "metadata": {
        "id": "kOT7x1_6U1hQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all PDF files in the specified directory\n",
        "pdf_file_names = get_files(pdf_folder_path)"
      ],
      "metadata": {
        "id": "GJqly7IXU1d2"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_to_text(pdf_path, output_folder):\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "\n",
        "    txt_file_path = os.path.join(output_folder, os.path.splitext(os.path.basename(pdf_path))[0] + \".txt\")\n",
        "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
        "        txt_file.write(text)"
      ],
      "metadata": {
        "id": "DT3wkxMyU1Y4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each PDF file to text and save in the text output folder\n",
        "for pdf_file_name in pdf_file_names:\n",
        "    pdf_file_path = os.path.join(pdf_folder_path, pdf_file_name)\n",
        "    convert_pdf_to_text(pdf_file_path, text_output_folder)"
      ],
      "metadata": {
        "id": "lm_dwOIoVKzh"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy's pre-trained model\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "I5YXnuqcPbuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from a text file\n",
        "def extract_text_from_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "u-PLzm0JWt27"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect structure using KMeans clustering\n",
        "def detect_structure(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract sentences or tokens for clustering\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "    # Convert text to features using CountVectorizer\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Optimize the number of clusters using the elbow method\n",
        "    scores = []\n",
        "    max_clusters = 100  # modify on the basis of your needs\n",
        "    for n_clusters in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        kmeans.fit(X)\n",
        "        score = silhouette_score(X, kmeans.labels_)\n",
        "        scores.append(score)\n",
        "\n",
        "    optimal_clusters = np.argmax(scores) + 2  # Adding 2 because we started from 2 clusters\n",
        "\n",
        "    # Choose the optimal clustering technique based on silhouette score\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "    agglomerative = AgglomerativeClustering(n_clusters=optimal_clusters)\n",
        "\n",
        "    kmeans.fit(X)\n",
        "    agglomerative.fit(X.toarray())\n",
        "\n",
        "    if silhouette_score(X, kmeans.labels_) > silhouette_score(X, agglomerative.labels_):\n",
        "        labels = kmeans.labels_\n",
        "    else:\n",
        "        labels = agglomerative.labels_\n",
        "\n",
        "    # Assign clusters to sentences\n",
        "    for i, label in enumerate(labels):\n",
        "        sentences[i] = (label, sentences[i])\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Function to process and detect structure in multiple text files\n",
        "def process_and_detect_structure(txt_folder):\n",
        "    detected_structures = {}\n",
        "\n",
        "    for filename in os.listdir(txt_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            txt_path = os.path.join(txt_folder, filename)\n",
        "            txt_text = extract_text_from_file(txt_path)\n",
        "            potential_structure = detect_structure(txt_text)\n",
        "            detected_structures[filename] = potential_structure\n",
        "\n",
        "    return detected_structures"
      ],
      "metadata": {
        "id": "TIWTMOgmLSbO"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_folder_path = '/content/drive/MyDrive/Colab_Notebooks/output_data'"
      ],
      "metadata": {
        "id": "aoWOVjfrYr3p"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process PDFs and detect potential structures\n",
        "detected_structures = process_and_detect_structure(text_folder_path)\n"
      ],
      "metadata": {
        "id": "mMHuIR8RL1Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the detected structures for each file\n",
        "for filename, structures in detected_structures.items():\n",
        "    print(f\"Detected Structures for {filename}:\")\n",
        "    for label, sentence in structures:\n",
        "        print(f\"Cluster {label}: {sentence}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "EZM91IMpL702"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, structure in detected_structures.items():\n",
        "    # Initialize variables to store extracted information\n",
        "    journal = \"\"\n",
        "    authors = \"\"\n",
        "    title = \"\"\n",
        "    year = \"\"\n",
        "    abstract = \"\"\n",
        "    corpus = \"\"\n",
        "\n",
        "    # Iterate through the structure to extract information\n",
        "    for label, sentence in structure:\n",
        "        if \"journal\" in sentence.lower():\n",
        "            journal = sentence\n",
        "        elif \"authors\" in sentence.lower():\n",
        "            authors = sentence.replace(\"Authors:\", \"\").strip()\n",
        "        elif \"title\" in sentence.lower():\n",
        "            title = sentence.replace(\"Title:\", \"\").strip()\n",
        "        elif re.search(r\"\\b(?:19|20)\\d{2}\\b\", sentence):\n",
        "            year = re.search(r\"\\b(?:19|20)\\d{2}\\b\", sentence).group()\n",
        "        elif \"abstract\" in sentence.lower():\n",
        "            abstract = sentence.replace(\"ABSTRACT\", \"\").strip()\n",
        "        elif \"introduction\" in sentence.lower():\n",
        "            corpus_start = structure.index((label, sentence)) + 1\n",
        "            break\n",
        "\n",
        "    # Extract corpus by concatenating sentences from corpus_start\n",
        "    corpus_sentences = [sentence for _, sentence in structure[corpus_start:]]\n",
        "    corpus = \" \".join(corpus_sentences)\n",
        "\n",
        "    # Print or store the extracted information\n",
        "    print(\"Filename:\", filename)\n",
        "    print(\"Journal:\", journal)\n",
        "    print(\"Authors:\", authors)\n",
        "    print(\"Title:\", title)\n",
        "    print(\"Year:\", year)\n",
        "    print(\"Abstract:\", abstract)\n",
        "    print(\"Corpus:\", corpus)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "ffuZyXJjc43J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Print or store the extracted information\n",
        "    print(\"Filename:\", filename)\n",
        "    print(\"Journal:\", journal)\n",
        "    print(\"Authors:\", authors)\n",
        "    print(\"Title:\", title)\n",
        "    print(\"Year:\", year)\n",
        "    print(\"Abstract:\", abstract)\n",
        "    print(\"Corpus:\", corpus)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "sm44Gh8BdyaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}